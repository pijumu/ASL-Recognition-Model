network_size: 2
layers:
  - activate_function: relu
    weights:
      - [0.047833333333333339, 0.078166666666666676]
      - [0, 0.080500000000000002]
      - [0.091000000000000011, 0.067666666666666667]
    bias: [0.005, 0]
  - activate_function: sigmoid
    weights:
      - [0.089599999999999999, 0.007000000000000001]
      - [0.063, 0.11340000000000001]
    bias: [0, 0.009]